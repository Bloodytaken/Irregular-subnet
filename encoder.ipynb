{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the Encoder Only \n",
    "\n",
    "We aim to make a script to verify the performance of TimeAwareEncoder. i.e., to train a encoder suitable for the dataset. \n",
    "  \n",
    "Assume the ${x}(t|t)$ is known (while in real conditions, it's not known), shown in the 5th column in the .csv file.  \n",
    "Typacially (in privious subnet work) na = nb =20.\n",
    "![flowchart](/Users/bloodytaken/graduate_project/deepSI/notes/flowchart.png)\n",
    "\n",
    "Design a method to test if 'TimeAwareEncoder' works by training a encoder using that network.  \n",
    "The ${x}(t|t)$ shoule be compared with the $\\hat{x}(t|t)$ to see if it is precise enough.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# x_true = torch.tensor(df['TrueState_'].values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder: \n",
    "$$x_0 = \\psi_\\theta (u_\\text{past}, y_\\text{past}, \\Delta t_\\text{past})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "\n",
    "class MLP_res_net_with_time(nn.Module):\n",
    "    '''Modified MLP_res_net with time interval (delta_t) as additional input.'''\n",
    "    def __init__(self, input_size: str | int | list, output_size: str | int | list, n_hidden_layers=3, n_hidden_nodes=128,\n",
    "                 activation=nn.GELU, zero_bias=True):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.scalar_output = output_size == 'scalar'\n",
    "\n",
    "        # Function to convert input shape\n",
    "        def to_num(s):\n",
    "            if isinstance(s, int):\n",
    "                return s\n",
    "            if s == 'scalar':\n",
    "                return 1\n",
    "            a = 1\n",
    "            for si in s:\n",
    "                a *= (1 if si == 'scalar' else si)\n",
    "            return a\n",
    "        \n",
    "        if isinstance(input_size, list):\n",
    "            input_size = sum(to_num(s) for s in input_size)\n",
    "\n",
    "        output_size = 1 if self.scalar_output else output_size\n",
    "        self.net_res = nn.Linear(input_size, output_size)\n",
    "\n",
    "        # Sequential MLP with residual connections\n",
    "        seq = [nn.Linear(input_size, n_hidden_nodes), activation()]\n",
    "        for _ in range(n_hidden_layers - 1):\n",
    "            seq.append(nn.Linear(n_hidden_nodes, n_hidden_nodes))\n",
    "            seq.append(activation())\n",
    "        seq.append(nn.Linear(n_hidden_nodes, output_size))\n",
    "        self.net_nonlin = nn.Sequential(*seq)\n",
    "\n",
    "        if zero_bias:\n",
    "            for m in self.modules(): \n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.constant_(m.bias, val=0)  # Set bias to zero\n",
    "\n",
    "\n",
    "    def forward(self, *ars):\n",
    "        if len(ars) == 1:\n",
    "            net_in = ars[0]\n",
    "            net_in = net_in.view(net_in.shape[0], -1)  # Adds a dim when needed\n",
    "        else:\n",
    "            net_in = torch.cat([a.view(a.shape[0], -1) for a in ars], dim=1)  # Flattens everything\n",
    "\n",
    "        out = self.net_nonlin(net_in) + self.net_res(net_in)\n",
    "        return out[:, 0] if self.scalar_output else out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "from networks import MLP_res_net\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 1. Normaliser\n",
    "# ----------------------------------------------------------------\n",
    "class Normalizer:\n",
    "    def __init__(self):\n",
    "        self.stats, self.log_flags = {}, {}\n",
    "\n",
    "    def fit_transform(self, name, tensor: torch.Tensor, use_log: bool = False):\n",
    "        if use_log:\n",
    "            tensor = torch.log1p(tensor)\n",
    "        mean, std = tensor.mean(), tensor.std()\n",
    "        self.stats[name] = dict(mean=mean, std=std)\n",
    "        self.log_flags[name] = use_log\n",
    "        return (tensor - mean) / std\n",
    "\n",
    "    def inverse_transform(self, name: str, norm_tensor: torch.Tensor):\n",
    "        if name not in self.stats:\n",
    "            raise ValueError(f\"No stats found for '{name}'\")\n",
    "        mean, std = self.stats[name]['mean'], self.stats[name]['std']\n",
    "        out = norm_tensor * std + mean\n",
    "        return torch.expm1(out) if self.log_flags[name] else out\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 2. Create windowed (past-L) data\n",
    "# ----------------------------------------------------------------\n",
    "def create_past_data(u, y, dt, true_state, past_len: int):\n",
    "    \"\"\"Return (u_past, y_past, dt_past, next_state).\"\"\"\n",
    "    dt_shifted = torch.cat([dt[1:], torch.tensor([[0.]], dtype=torch.float32)], dim=0)\n",
    "    u_past, y_past, dt_past, state_next = [], [], [], []\n",
    "\n",
    "    for i in range(past_len, len(u) - 1):\n",
    "        u_past.append(torch.flip(u[i - past_len + 1:i + 1], dims=[0]).flatten())\n",
    "        y_past.append(torch.flip(y[i - past_len + 1:i + 1], dims=[0]).flatten())\n",
    "\n",
    "        dt_window = torch.flip(dt_shifted[i - past_len + 1:i + 1], dims=[0]).flatten()\n",
    "        dt_window[dt_window == 0] = 1e-9          # avoid exact zeros\n",
    "        dt_past.append(dt_window)\n",
    "\n",
    "        state_next.append(true_state[i + 1])\n",
    "\n",
    "    return (torch.stack(u_past),\n",
    "            torch.stack(y_past),\n",
    "            torch.stack(dt_past),\n",
    "            torch.stack(state_next))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 3. Load data and build DataLoaders\n",
    "# ----------------------------------------------------------------\n",
    "# CSV_PATH = '/Users/bloodytaken/graduate_project/data/MSD_linear_noiseless_k_040.csv'\n",
    "df = pd.read_csv('/Users/bloodytaken/graduate_project/data/MSD/mass_spring_damper_data_state.csv')\n",
    "\n",
    "u_raw = torch.tensor(df['Input'].values,  dtype=torch.float32).view(-1, 1)\n",
    "y_raw = torch.tensor(df['Output'].values, dtype=torch.float32).view(-1, 1)\n",
    "dt_raw = torch.tensor(df['Delta_t'].values, dtype=torch.float32).view(-1, 1)\n",
    "state  = torch.tensor(df[['TrueState_1', 'TrueState_2']].values, dtype=torch.float32)\n",
    "\n",
    "normal = Normalizer()\n",
    "u_norm = normal.fit_transform('u', u_raw)\n",
    "y_norm = normal.fit_transform('y', y_raw)\n",
    "\n",
    "nx, nb, na = 2, 5, 5     # state dim, input taps, output taps\n",
    "u_past, y_past, dt_past, state_next = create_past_data(\n",
    "        u_norm, y_norm, dt_raw, state, past_len=max(nb, na))\n",
    "\n",
    "# ---- split: 70 % train / 15 % val / 15 % test\n",
    "N = len(u_past)\n",
    "train_end, val_end = int(0.60 * N), int(0.75 * N)\n",
    "\n",
    "train_ds = TensorDataset(u_past[:train_end], y_past[:train_end],\n",
    "                         dt_past[:train_end], state_next[:train_end])\n",
    "val_ds   = TensorDataset(u_past[train_end:val_end], y_past[train_end:val_end],\n",
    "                         dt_past[train_end:val_end], state_next[train_end:val_end])\n",
    "test_ds  = TensorDataset(u_past[val_end:], y_past[val_end:],\n",
    "                         dt_past[val_end:], state_next[val_end:])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 4. Instantiate models and optimisers\n",
    "# ----------------------------------------------------------------\n",
    "model_time = MLP_res_net_with_time(\n",
    "    input_size=[(nb, 1), (na, 1), (max(nb, na), 1)], output_size=nx)\n",
    "model_plain = MLP_res_net(\n",
    "    input_size=[(nb, 1), (na, 1)], output_size=nx)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "lr = 1e-3\n",
    "optim_time  = optim.Adam(model_time.parameters(),  lr=lr, weight_decay=1e-4)\n",
    "optim_plain = optim.Adam(model_plain.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 5. Training helper with NRMS metric\n",
    "# ----------------------------------------------------------------\n",
    "def compute_vector_nrms(y_true: torch.Tensor, y_pred: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Compute NRMS using vector-wise RMSE and vector-wise standard deviation.\n",
    "    \"\"\"\n",
    "    residual = y_true - y_pred\n",
    "    rmse = torch.sqrt(torch.mean(residual.pow(2)))                    # scalar\n",
    "    std = torch.std(y_true.view(-1))                                  # flatten all dims\n",
    "    return (rmse / std).item()\n",
    "\n",
    "\n",
    "def train_model(model, optimiser, name):\n",
    "    best_val = float('inf')\n",
    "    early_cnt = 0\n",
    "    sched = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimiser, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "    for epoch in trange(30000, desc=f'{name} training', dynamic_ncols=True):\n",
    "        # ---- train\n",
    "        model.train()\n",
    "        tr_loss = 0.0\n",
    "        for u_b, y_b, dt_b, s_b in train_loader:\n",
    "            optimiser.zero_grad()\n",
    "            pred = model(u_b, y_b, dt_b) if 'time' in name else model(u_b, y_b)\n",
    "            loss = criterion(pred, s_b)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            tr_loss += loss.item()\n",
    "        tr_loss /= len(train_loader)\n",
    "\n",
    "        # ---- val\n",
    "        model.eval()\n",
    "        v_loss = 0.0\n",
    "        y_true_all, y_pred_all = [], []\n",
    "        with torch.no_grad():\n",
    "            for u_b, y_b, dt_b, s_b in val_loader:\n",
    "                pred = model(u_b, y_b, dt_b) if 'time' in name else model(u_b, y_b)\n",
    "                y_true_all.append(s_b)\n",
    "                y_pred_all.append(pred)\n",
    "                v_loss += criterion(pred, s_b).item()\n",
    "        v_loss /= len(val_loader)\n",
    "\n",
    "        sched.step(v_loss)\n",
    "\n",
    "        # Compute NRMS (on full val set)\n",
    "        y_true = torch.cat(y_true_all, dim=0)\n",
    "        y_pred = torch.cat(y_pred_all, dim=0)\n",
    "        nrms_val = compute_vector_nrms(y_true, y_pred)\n",
    "\n",
    "\n",
    "        if v_loss < best_val:\n",
    "            best_val = v_loss\n",
    "            torch.save(model.state_dict(), f'best_{name}.pth')\n",
    "            early_cnt = 0\n",
    "        else:\n",
    "            early_cnt += 1\n",
    "\n",
    "        if early_cnt >= 5000:\n",
    "            print(f'Early stop: {name}')\n",
    "            break\n",
    "\n",
    "        if (epoch + 1) % 100 == 0 or epoch == 0:\n",
    "            print(f\"[{name}] Epoch {epoch+1:03d}  train {tr_loss:.4e}  val {v_loss:.4e}  NRMS: {nrms_val:.4f}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 6. Run training\n",
    "# ----------------------------------------------------------------\n",
    "train_model(model_time,  optim_time,  'MLP_res_net_with_time')\n",
    "train_model(model_plain, optim_plain, 'MLP_res_net')\n",
    "\n",
    "# reload best weights\n",
    "model_time.load_state_dict(torch.load('best_MLP_res_net_with_time.pth'))\n",
    "model_plain.load_state_dict(torch.load('best_MLP_res_net.pth'))\n",
    "model_time.eval();  model_plain.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test results visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "true_s, pred_time, pred_plain = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for u_b, y_b, dt_b, s_b in test_loader:\n",
    "        true_s.append(s_b)\n",
    "        pred_time.append(model_time(u_b, y_b, dt_b))\n",
    "        pred_plain.append(model_plain(u_b, y_b))\n",
    "\n",
    "true_s     = torch.cat(true_s).cpu().numpy()   # [N, 2]\n",
    "pred_time  = torch.cat(pred_time).cpu().numpy()\n",
    "pred_plain = torch.cat(pred_plain).cpu().numpy()\n",
    "idx = np.arange(len(true_s))\n",
    "\n",
    "fig1, ax1 = plt.subplots(2, 1, figsize=(8, 6), sharex=True)\n",
    "\n",
    "# --- ① Position subplot\n",
    "ax1[0].plot(idx, true_s[:, 0],        'k-',  lw=1, label='measurement $x$')\n",
    "ax1[0].plot(idx, pred_time[:, 0],     'C1-', lw=1, label='prediction $x$ (with Δt)')\n",
    "ax1[0].plot(idx, pred_time[:, 0] - true_s[:, 0],\n",
    "             ls='--', lw=1, alpha=1, label='x residual')\n",
    "ax1[0].set_ylabel('position x')\n",
    "ax1[0].legend(loc='upper right')\n",
    "\n",
    "# --- ② Velocity subplot\n",
    "ax1[1].plot(idx, true_s[:, 1],        'k-',  lw=1, label='ground-truth ẋ')\n",
    "ax1[1].plot(idx, pred_time[:, 1],     'C1-', lw=1, label='pred ẋ (with Δt)')\n",
    "ax1[1].plot(idx, pred_time[:, 1] - true_s[:, 1],\n",
    "             ls='--', lw=1, alpha=1, label='ẋ residual')\n",
    "ax1[1].set_ylabel('velocity ẋ')\n",
    "ax1[1].set_xlabel('time index')\n",
    "ax1[1].legend(loc='upper right')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual arrays\n",
    "res_plain_x = pred_plain[:, 0] - true_s[:, 0]\n",
    "res_time_x  = pred_time[:, 0]  - true_s[:, 0]\n",
    "plt.figure(figsize=(5,3))           # 4:3 ratio\n",
    "plt.axhline(0, color='grey', lw=0.5)\n",
    "\n",
    "# bottom layer  ─ plain\n",
    "plt.plot(idx, res_plain_x, \n",
    "         lw=1.0, alpha=0.6, zorder=1, label='plain Encoder')\n",
    "\n",
    "# top layer  ─ with Δt\n",
    "plt.plot(idx, res_time_x,  \n",
    "         lw=1.0, alpha=0.9, zorder=5, label='Encoder with Δt')\n",
    "\n",
    "plt.xlabel('time index')\n",
    "plt.ylabel('position residual  (x)')\n",
    "# plt.title('Residual comparison')\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_s_tensor = torch.tensor(true_s)\n",
    "pred_time_tensor = torch.tensor(pred_time)\n",
    "pred_plain_tensor = torch.tensor(pred_plain)\n",
    "\n",
    "nrms_time = compute_vector_nrms(true_s_tensor, pred_time_tensor)\n",
    "nrms_plain = compute_vector_nrms(true_s_tensor, pred_plain_tensor)\n",
    "\n",
    "print(f\"NRMS (with Δt): {nrms_time:.4f}\")\n",
    "print(f\"NRMS (plain):   {nrms_plain:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "k=0\n",
    "NRMS (with Δt): 0.0001\n",
    "NRMS (plain):   0.0001\n",
    "\n",
    "k=0.1\n",
    "NRMS (with Δt): 0.0411\n",
    "NRMS (plain):   0.2272\n",
    "\n",
    "k=0.5\n",
    "NRMS (with Δt): 0.0635\n",
    "NRMS (plain):   0.5889\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepSI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
